{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initialize","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nimport copy\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split\n\nhas_mps = torch.backends.mps.is_built()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_model = None\n        self.best_loss = None\n        self.counter = 0\n        self.status = \"\"\n\n    def __call__(self, model, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.best_model = copy.deepcopy(model.state_dict())\n        elif self.best_loss - val_loss >= self.min_delta:\n            self.best_model = copy.deepcopy(model.state_dict())\n            self.best_loss = val_loss\n            self.counter = 0\n            self.status = f\"Improvement found, counter reset to {self.counter}\"\n        else:\n            self.counter += 1\n            self.status = f\"No improvement in the last {self.counter} epochs\"\n            if self.counter >= self.patience:\n                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n                if self.restore_best_weights:\n                    model.load_state_dict(self.best_model)\n                return True\n        return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Here ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#onek kaj ache ekhane\n\n# üéØ One-hot encode target\nx_columns = df.columns.drop(\"product\").drop(\"id\")\nx = df[x_columns].values\ndummies = pd.get_dummies(df[\"product\"], dtype=int)\ny = dummies.values\ny_class = np.argmax(y, axis=1)  # Needed for StratifiedKFold\n\n# ‚úÖ Convert once to tensors\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nx_tensor = torch.tensor(x, dtype=torch.float32, device=device)\ny_tensor = torch.tensor(y_class, dtype=torch.long, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üß† Model architecture\ndef build_model(input_dim, output_dim):\n    return torch.compile(nn.Sequential(\n        nn.Linear(input_dim, 64),\n        nn.ReLU(),\n        nn.Linear(64, 32),\n        nn.ReLU(),\n        nn.Linear(32, output_dim),\n        nn.Softmax(dim=1),\n    ), backend=\"aot_eager\").to(device)\n\n# üìä Tracking\noos_y = []\noos_pred = []\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold = 0\n\nfor train_idx, test_idx in kf.split(x_tensor.cpu(), y_tensor.cpu()):\n    fold += 1\n    print(f\"\\nüìÇ Fold #{fold}\")\n\n    # Split data\n    x_train, x_test = x_tensor[train_idx], x_tensor[test_idx]\n    y_train, y_test = y_tensor[train_idx], y_tensor[test_idx]\n\n    train_ds = TensorDataset(x_train, y_train)\n    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n\n    # Model setup\n    model = build_model(x.shape[1], y.shape[1])\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    es = EarlyStopping(patience=10)\n    EPOCHS = 100\n\n    for epoch in range(1, EPOCHS + 1):\n        model.train()\n        for xb, yb in train_loader:\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n\n        # Validation loss\n        model.eval()\n        with torch.no_grad():\n            val_logits = model(x_test)\n            val_loss = criterion(val_logits, y_test)\n\n        print(f\"Epoch {epoch} - Val Loss: {val_loss.item():.4f} ({es.status})\")\n        if es(model, val_loss.item()):\n            print(\"‚õî Early stopping triggered.\")\n            break\n\n    # üîÅ Load best weights\n    model.load_state_dict(es.best_model_state)\n\n    # üéØ Final prediction\n    model.eval()\n    with torch.no_grad():\n        final_probs = model(x_test)\n        final_preds = torch.argmax(final_probs, dim=1)\n\n    oos_y.append(y_test.cpu().numpy())\n    oos_pred.append(final_preds.cpu().numpy())\n\n    fold_acc = metrics.accuracy_score(y_test.cpu().numpy(), final_preds.cpu().numpy())\n    print(f\"‚úÖ Fold Accuracy: {fold_acc:.4f}\")\n\n# üßÆ Final accuracy\noos_y = np.concatenate(oos_y)\noos_pred = np.concatenate(oos_pred)\nfinal_acc = metrics.accuracy_score(oos_y, oos_pred)\nprint(f\"\\nüèÅ Final Overall Accuracy: {final_acc:.4f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}